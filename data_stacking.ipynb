{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ff4b74b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 研究意义和目的\n",
    "探索现在活跃用户中的有流失风险的用户，使用机器学习的方法进行模型预测，提供给公司对用户流失行为的预判。\n",
    "\n",
    "# 流失用户数据判定 churn\n",
    "## 规则1\n",
    "fixed_month > 3\n",
    "version < 8.21.0\n",
    "\n",
    "half: age_section == '90后' \n",
    "half: age_section == '80后' \n",
    "\n",
    "## 规则2 \n",
    "fixed_month > 3\n",
    "source_channel: except carpool "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8f562c0",
   "metadata": {},
   "source": [
    "- 数据处理[https://blog.csdn.net/erinapple/article/details/81174918]\n",
    "- stacking[https://blog.csdn.net/song430/article/details/90232554]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2568e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#常用工具库\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#算法辅助 & 数据\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#算法（单一学习器）\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNNC\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.linear_model import LogisticRegression as LogiR\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "#融合模型\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb958dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "['user_id', 'cid', 'device_type', 'fix_month', 'gender', 'current_role', 'last_lat', 'last_lon', 'hometown', 'profession', 'source_channel', 'work_city_id', 'home_city_id', 'work_lon', 'home_lat', 'work_lat', 'home_lon', 'age_section', 'age', 'version', 'model', 'label']\n",
      "['23662216', 'fe0f10d1-2481-47a9-937d-e1e6a367e526', '2', '13', '1', '1', '0', '0', '', '总务部', 'carpool', '352', '352', '108.64134', '34.37028', '34.30623', '108.77005', '90后', '24', '8.23.0', 'Xiaomi Mi 10', '0']\n",
      "27801\n",
      "23183\n",
      "4618\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "features = [\"user_id\",\"cid\",\"device_type\",\"fix_month\",\"gender\",\"current_role\",\"last_lat\",\"last_lon\",\"hometown\",\"profession\",\"source_channel\",\"work_city_id\",\"home_city_id\",\"work_lon\",\"home_lat\",\"work_lat\",\"home_lon\",\"age_section\",\"age\",\"version\",\"model\",\"label\"]\n",
    "print(len(features))\n",
    "print(features)\n",
    "label_file = \"data/0403_churn_2.csv\"\n",
    "with codecs.open(label_file) as f:\n",
    "    for line in f.readlines():\n",
    "        data = line.strip().split(\"\\t\")\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "# create a DataFrame from the dictionary\n",
    "data = pd.read_csv(label_file, delimiter=\"\\t\", names=features).drop(\"cid\", axis=1)\n",
    "\n",
    "# set the column data types\n",
    "dtype = {\n",
    "    'user_id': 'int64',\n",
    "    'device_type': 'int8',\n",
    "    'fix_month': 'int8',\n",
    "    'gender': 'int8',\n",
    "    'current_role': 'int8',\n",
    "    'last_lat': 'float32',\n",
    "    'last_lon': 'float32',\n",
    "    'hometown': 'category',\n",
    "    'profession': 'category',\n",
    "    'source_channel': 'category',\n",
    "    'work_city_id': 'int8',\n",
    "    'home_city_id': 'int8',\n",
    "    'work_lon': 'float32',\n",
    "    'home_lat': 'float32',\n",
    "    'work_lat': 'float32',\n",
    "    'home_lon': 'float32',\n",
    "    'age_section': 'category',\n",
    "    'age': 'int8',\n",
    "    'version': 'category',\n",
    "    'model': 'category',\n",
    "    'label': 'int8'\n",
    "}\n",
    "\n",
    "# apply the column data types to the DataFrame\n",
    "df = data.astype(dtype)\n",
    "print(len(df))\n",
    "\n",
    "selected_rows_0 = df[df['label'] == 0]\n",
    "print(len(selected_rows_0))\n",
    "\n",
    "selected_rows_1 = df[df['label'] == 1]\n",
    "print(len(selected_rows_1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46313f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  22240\n",
      "Test set size:  5561\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# display the sizes of the two subsets\n",
    "print(\"Training set size: \", len(train_df))\n",
    "print(\"Test set size: \", len(test_df))\n",
    "\n",
    "train_df.head(2)\n",
    "\n",
    "feature_columns = [\"user_id\",\"device_type\",\"fix_month\",\"gender\",\"current_role\",\"last_lat\",\"last_lon\",\"hometown\",\"profession\",\"source_channel\",\"work_city_id\",\"home_city_id\",\"work_lon\",\"home_lat\",\"work_lat\",\"home_lon\",\"age_section\",\"age\",\"version\",\"model\"]\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_train.head(2)\n",
    "\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### xgb\n",
    "\n",
    "xgb_params = {'eta': 0.005, \n",
    "              'max_depth': 10, \n",
    "              'subsample': 0.8, \n",
    "              'colsample_bytree': 0.8, \n",
    "              'objective': 'reg:squarederror', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': 8}#xgb的参数，可以自己改\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1412)#5折交叉验证\n",
    "oof_xgb = np.zeros(len(train_df))#用于存放训练集的预测\n",
    "predictions_xgb = np.zeros(len(test_df))#用于存放测试集的预测\n",
    "\n",
    "for fold_, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print(f'Fold {fold_+1}:')\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    trn_data = xgb.DMatrix(X_train_fold, label=y_train_fold, enable_categorical=True)\n",
    "    val_data = xgb.DMatrix(X_val_fold, label=y_val_fold, enable_categorical=True)\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=100, params=xgb_params)#80%用于训练过程\n",
    "\n",
    "    oof_xgb[val_index] = clf.predict(xgb.DMatrix(X_val_fold, enable_categorical=True), ntree_limit=clf.best_ntree_limit)#预测20%的验证集\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(X_test, enable_categorical=True), ntree_limit=clf.best_ntree_limit) / kf.n_splits#预测测试集，并且取平均\n",
    "\n",
    "print(\"CV score train: {:<8.8f}\".format(mean_squared_error(oof_xgb, y_train)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CV score test: {:<8.8f}\".format(mean_squared_error(predictions_xgb, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e326666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's l2: 0.0459139\tvalid_1's l2: 0.0713996\n",
      "Early stopping, best iteration is:\n",
      "[234]\ttraining's l2: 0.0418333\tvalid_1's l2: 0.0707173\n",
      "(4448,)\n",
      "Fold 2:\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=30 will be ignored. Current value: min_data_in_leaf=30\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's l2: 0.0459427\tvalid_1's l2: 0.070493\n",
      "Early stopping, best iteration is:\n",
      "[243]\ttraining's l2: 0.0409975\tvalid_1's l2: 0.0697725\n",
      "(4448,)\n",
      "Fold 3:\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=30 will be ignored. Current value: min_data_in_leaf=30\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's l2: 0.0463828\tvalid_1's l2: 0.0679861\n",
      "Early stopping, best iteration is:\n",
      "[262]\ttraining's l2: 0.0395737\tvalid_1's l2: 0.0673181\n",
      "(4448,)\n",
      "Fold 4:\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=30 will be ignored. Current value: min_data_in_leaf=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttraining's l2: 0.0456038\tvalid_1's l2: 0.0709915\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's l2: 0.0394415\tvalid_1's l2: 0.0699471\n",
      "(4448,)\n",
      "Fold 5:\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=30 will be ignored. Current value: min_data_in_leaf=30\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "d:\\resources\\conda\\envs\\stacking_env\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's l2: 0.0462596\tvalid_1's l2: 0.0704751\n",
      "Early stopping, best iteration is:\n",
      "[293]\ttraining's l2: 0.0367502\tvalid_1's l2: 0.0690607\n",
      "(4448,)\n",
      "CV score train: 0.06936316\n",
      "CV score test: 0.06875750\n"
     ]
    }
   ],
   "source": [
    "##### lgb\n",
    "param = {'num_leaves': 120,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'mse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1}#模型参数，可以修改\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2018)#5折交叉验证\n",
    "oof_lgb = np.zeros(len(train_df))#存放训练集的预测结果\n",
    "predictions_lgb = np.zeros(len(test_df))#存放测试集的预测结果\n",
    "\n",
    "for fold_, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print(f'Fold {fold_+1}:')\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    trn_data = lgb.Dataset(X_train_fold, y_train_fold)#80%的训练集用于训练\n",
    "    val_data = lgb.Dataset(X_val_fold, y_val_fold)#20%的训练集做验证集\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 100)#训练过程\n",
    "    \n",
    "    result = clf.predict(X_val_fold, num_iteration=clf.best_iteration)#对验证集得到预测结果\n",
    "    print(result.shape)\n",
    "    oof_lgb[val_index] = result\n",
    "    predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / kf.n_splits#对测试集5次取平均值\n",
    "\n",
    "print(\"CV score train: {:<8.8f}\".format(mean_squared_error(oof_lgb, y_train)))\n",
    "print(\"CV score test: {:<8.8f}\".format(mean_squared_error(predictions_lgb, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f613d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将lgb和xgb的结果进行stacking（叠加）\n",
    "train_stack = np.vstack([oof_lgb,oof_xgb]).transpose()#训练集2列特征\n",
    "test_stack = np.vstack([predictions_lgb, predictions_xgb]).transpose()#测试集2列特征\n",
    "#贝叶斯分类器也使用交叉验证的方法，5折，重复2次，主要是避免过拟合\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2018)\n",
    "oof_stack = np.zeros(train_stack.shape[0])#存放训练集中验证集的预测结果\n",
    "predictions = np.zeros(test_stack.shape[0])#存放测试集的预测结果\n",
    "\n",
    "#enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack,y_train)):#target就是每一行样本的标签值\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    \n",
    "    trn_data, trn_y = train_stack[trn_idx], y_train.iloc[trn_idx].values#划分训练集的80%\n",
    "    val_data, val_y = train_stack[val_idx], y_train.iloc[val_idx].values#划分训练集的20%做验证集\n",
    "    \n",
    "    clf_3 = BayesianRidge()\n",
    "    clf_3.fit(trn_data, trn_y)#贝叶斯训练过程，sklearn中的。\n",
    "    \n",
    "    oof_stack[val_idx] = clf_3.predict(val_data)#对验证集有一个预测，用于后面计算模型的偏差\n",
    "    predictions += clf_3.predict(test_stack) / 10 #对测试集的预测，除以10是因为5折交叉验证重复了2次\n",
    "    \n",
    "mean_squared_error(y_train.values, oof_stack)#计算出模型在训练集上的均方误差\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(y_train.values, oof_stack)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1280284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_file = \"data/0403_churn_2.csv\"\n",
    "# create a DataFrame from the dictionary\n",
    "data = pd.read_csv(label_file, delimiter=\"\\t\", names=[\"user_id\",\"cid\",\"device_type\",\"fix_month\",\"gender\",\"current_role\",\"last_lat\",\"last_lon\",\"hometown\",\"profession\",\"source_channel\",\"work_city_id\",\"home_city_id\",\"work_lon\",\"home_lat\",\"work_lat\",\"home_lon\",\"age_section\",\"age\",\"version\",\"model\",\"label\"]).drop(\"cid\", axis=1)\n",
    "\n",
    "# one-hot encode the categorical features\n",
    "cat_features = ['hometown', 'profession', 'source_channel', 'age_section', 'version', 'model']\n",
    "df = pd.get_dummies(data, columns=cat_features)\n",
    "\n",
    "# set the column data types\n",
    "dtype = {\n",
    "    'user_id': 'int64',\n",
    "    'device_type': 'int8',\n",
    "    'fix_month': 'int8',\n",
    "    'gender': 'int8',\n",
    "    'current_role': 'int8',\n",
    "    'last_lat': 'float32',\n",
    "    'last_lon': 'float32',\n",
    "    'work_city_id': 'int32',\n",
    "    'home_city_id': 'int32',\n",
    "    'work_lon': 'float32',\n",
    "    'home_lat': 'float32',\n",
    "    'work_lat': 'float32',\n",
    "    'home_lon': 'float32',\n",
    "    'age': 'int8',\n",
    "    'label': 'int8'\n",
    "}\n",
    "\n",
    "# apply the column data types to the DataFrame\n",
    "df = df.astype(dtype)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "data_message = 'X_train.shape={}, X_test.shape={}'.format(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a90b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27801, 7729)\n",
      "27801\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12a087be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "label_file = \"data/0403_churn_2.csv\"\n",
    "data = pd.read_csv(label_file, delimiter=\"\\t\", names=[\"user_id\",\"cid\",\"device_type\",\"fix_month\",\"gender\",\"current_role\",\"last_lat\",\"last_lon\",\"hometown\",\"profession\",\"source_channel\",\"work_city_id\",\"home_city_id\",\"work_lon\",\"home_lat\",\"work_lat\",\"home_lon\",\"age_section\",\"age\",\"version\",\"model\",\"label\"]).drop(\"cid\", axis=1)\n",
    "\n",
    "# set the column data types\n",
    "dtype = {\n",
    "    'user_id': 'int64',\n",
    "    'device_type': 'int8',\n",
    "    'fix_month': 'int8',\n",
    "    'gender': 'int8',\n",
    "    'current_role': 'int8',\n",
    "    'last_lat': 'float32',\n",
    "    'last_lon': 'float32',\n",
    "    'hometown': 'category',\n",
    "    'profession': 'category',\n",
    "    'source_channel': 'category',\n",
    "    'work_city_id': 'int8',\n",
    "    'home_city_id': 'int8',\n",
    "    'work_lon': 'float32',\n",
    "    'home_lat': 'float32',\n",
    "    'work_lat': 'float32',\n",
    "    'home_lon': 'float32',\n",
    "    'age_section': 'category',\n",
    "    'age': 'int8',\n",
    "    'version': 'category',\n",
    "    'model': 'category',\n",
    "    'label': 'int8'\n",
    "}\n",
    "\n",
    "# apply the column data types to the DataFrame\n",
    "df = data.astype(dtype)\n",
    "df['hometown'] = df['hometown'].cat.add_categories('NA')\n",
    "df['hometown'] = df['hometown'].fillna('NA')\n",
    "\n",
    "if type(df) == pd.DataFrame:\n",
    "    df = df.values\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776adffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import catboost as cgb\n",
    "\n",
    "params = {\n",
    "            'learning_rate': 0.05,\n",
    "            'eval_metric': 'AUC',\n",
    "            'depth': 8,\n",
    "            'logging_level': 'Info',\n",
    "            'loss_function': 'Logloss',\n",
    "            'train_dir': 'model/cgb_record/',\n",
    "            'thread_count': 6\n",
    "        }\n",
    "max_round = 1\n",
    "cv_folds = 2\n",
    "seed = 3\n",
    "save_model_path = 'model/cgb.model'\n",
    "\n",
    "dtrain = cgb.Pool(X_train, label=y_train)\n",
    "cv_result = cgb.cv(dtrain, params, num_boost_round=max_round, nfold=cv_folds, seed=seed, logging_level='Verbose')\n",
    "\n",
    "print(cv_result.keys())\n",
    "for key, value in cv_result.items():\n",
    "    print(key, value)\n",
    "\n",
    "auc_test_mean = cv_result['test-AUC-mean']\n",
    "best_round = np.argmax(auc_test_mean)\n",
    "best_auc = np.max(auc_test_mean)  # 最好的 auc 值\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stacking_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4558dd3f36802e4cd3159574cc0d8d7c72aecc4239a1f962b3bf9d954215d59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
